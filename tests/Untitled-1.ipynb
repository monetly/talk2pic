{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dictmanage/miniconda3/envs/talk2pic/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading pipeline components...:  14%|█▍        | 1/7 [00:00<00:03,  1.90it/s]You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  9.00it/s]it/s]\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:01<00:00,  6.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model LoRA has been loaded: /home/dictmanage/.cache/modelscope/hub/models/lip421/ertongchahuaMAILANDFLUX/\n",
      "Model has been loaded: /home/dictmanage/.cache/modelscope/hub/models/black-forest-labs/FLUX___1-dev/\n"
     ]
    }
   ],
   "source": [
    "from src.trans import trans\n",
    "model_id=\"/home/dictmanage/.cache/modelscope/hub/models/black-forest-labs/FLUX___1-dev/\"\n",
    "lora_id=\"/home/dictmanage/.cache/modelscope/hub/models/yiwanji/FLUX_xiao_hong_shu_ji_zhi_zhen_shi_V2/\"\n",
    "lora_id1=\"/home/dictmanage/.cache/modelscope/hub/models/ChaosMY/MYkawaii4MJ/\"\n",
    "lora_id2=\"/home/dictmanage/.cache/modelscope/hub/models/lip421/ertongchahuaMAILANDFLUX/\"\n",
    "key=\"sk-qzvagazrsjcgrkqfmynotwtovonbqxcrfhmgpleutfovldgo\"\n",
    "a=trans(key,model_id,lora_id2,\"park\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1 dialogues...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['shading. color palette : warm yellows, soft blues, and earthy greens.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating image (Attempt 1)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:44<00:00,  3.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: park/image_000_attempt0_1742190712.png\n",
      "Image evaluation score: 8/10\n",
      "❌ Too many issues detected (score 8/10). Refining prompt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['fingers slightly curved to convey friendliness. the school gate buzzes with activity — students chatting in small groups, cars carefully dropping off peers, and the morning sun casting soft, balanced light that enhances the scene without harsh shadows. the perspective is carefully composed to ensure proportions and angles are accurate, with the school gate receding naturally into the background. illustration style : clean, semi - realistic lines with smooth, soft shading to add depth and dimension. color palette : warm yellows for the sunlight, soft blues for the sky and accents, and earthy greens for the surroundings to create a harmonious, inviting atmosphere. this revised prompt addresses the issues by specifying natural hand positions, balanced lighting, and accurate perspective while maintaining the original intent and style.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating image (Attempt 2)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:39<00:00,  3.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: park/image_000_attempt1_1742190819.png\n",
      "Image evaluation score: 5/10\n",
      "✅ Image is acceptable with score 5/10.\n"
     ]
    }
   ],
   "source": [
    "a.batch_generate(\"/home/dictmanage/liuxh/talk2pic/tests/park/park.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'flux' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m####flux#####\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m==\u001b[33m'\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     sd = \u001b[43mflux\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33mruler\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m     model_id=\u001b[33m\"\u001b[39m\u001b[33m/home/dictmanage/.cache/modelscope/hub/models/tensorart/stable-diffusion-3.5-medium-turbo/\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      5\u001b[39m     model_id1=\u001b[33m\"\u001b[39m\u001b[33m/home/dictmanage/.cache/modelscope/hub/models/black-forest-labs/FLUX.1-dev/\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'flux' is not defined"
     ]
    }
   ],
   "source": [
    "####flux#####\n",
    "if __name__=='__main__':\n",
    "    sd = flux(\"ruler\")\n",
    "    model_id=\"/home/dictmanage/.cache/modelscope/hub/models/tensorart/stable-diffusion-3.5-medium-turbo/\"\n",
    "    model_id1=\"/home/dictmanage/.cache/modelscope/hub/models/black-forest-labs/FLUX.1-dev/\"\n",
    "    adapter_id = \"/home/dictmanage/.cache/modelscope/hub/models/yiwanji/FLUX_xiao_hong_shu_ji_zhi_zhen_shi_V2/\"\n",
    "    Child_Il=\"/home/dictmanage/.cache/modelscope/hub/models/lip421/ertongchahuaMAILANDFLUX/\"\n",
    "    sd.load_model(model_id1,Child_Il)\n",
    "    sd.generated_pic('how are you my friend')\n",
    "    sd.release_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 3/3 [00:00<00:00, 19.22it/s]\n",
      "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "Loading pipeline components...: 100%|██████████| 5/5 [00:00<00:00,  8.48it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 42.06 MiB is free. Process 21724 has 352.70 MiB memory in use. Process 2999896 has 2.86 GiB memory in use. Including non-PyTorch memory, this process has 20.27 GiB memory in use. Of the allocated memory 19.90 GiB is allocated by PyTorch, and 30.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdiffusers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_image\n\u001b[32m      4\u001b[39m pipe_prior_redux = FluxPriorReduxPipeline.from_pretrained(rudex_id, torch_dtype=torch.bfloat16).to(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m pipe = \u001b[43mFluxPipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext_encoder\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext_encoder_2\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbfloat16\u001b[49m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m image = load_image(\u001b[33m\"\u001b[39m\u001b[33mhttps://huggingface.co/datasets/huggingface/documentation-images/resolve/main/robot.png\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m pipe_prior_output = pipe_prior_redux(image)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/talk2pic/lib/python3.12/site-packages/diffusers/pipelines/pipeline_utils.py:461\u001b[39m, in \u001b[36mDiffusionPipeline.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    459\u001b[39m     module.to(device=device)\n\u001b[32m    460\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_loaded_in_4bit_bnb \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_loaded_in_8bit_bnb:\n\u001b[32m--> \u001b[39m\u001b[32m461\u001b[39m     \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    464\u001b[39m     module.dtype == torch.float16\n\u001b[32m    465\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(device) \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    466\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m silence_dtype_warnings\n\u001b[32m    467\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_offloaded\n\u001b[32m    468\u001b[39m ):\n\u001b[32m    469\u001b[39m     logger.warning(\n\u001b[32m    470\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPipelines loaded with `dtype=torch.float16` cannot run with `cpu` device. It\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    471\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m is not recommended to move them to `cpu` as running them will fail. Please make\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    474\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m `torch_dtype=torch.float16` argument, or use another device for inference.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    475\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/talk2pic/lib/python3.12/site-packages/diffusers/models/modeling_utils.py:1077\u001b[39m, in \u001b[36mModelMixin.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1072\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m is_bitsandbytes_version(\u001b[33m\"\u001b[39m\u001b[33m<\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m0.43.2\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1073\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1074\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mCalling `to()` is not supported for `4-bit` quantized models with the installed version of bitsandbytes. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1075\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe current device is `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.device\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`. If you intended to move the model, please install bitsandbytes >= 0.43.2.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1076\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1077\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/talk2pic/lib/python3.12/site-packages/torch/nn/modules/module.py:1343\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1340\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1341\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/talk2pic/lib/python3.12/site-packages/torch/nn/modules/module.py:903\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    901\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    902\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m903\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    906\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    907\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    908\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    913\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    914\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/talk2pic/lib/python3.12/site-packages/torch/nn/modules/module.py:903\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    901\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    902\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m903\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    906\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    907\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    908\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    913\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    914\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/talk2pic/lib/python3.12/site-packages/torch/nn/modules/module.py:903\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    901\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    902\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m903\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    906\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    907\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    908\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    913\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    914\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/talk2pic/lib/python3.12/site-packages/torch/nn/modules/module.py:930\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    926\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    927\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    928\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    929\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    931\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    933\u001b[39m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/talk2pic/lib/python3.12/site-packages/torch/nn/modules/module.py:1329\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1322\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t.dim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m):\n\u001b[32m   1323\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1324\u001b[39m             device,\n\u001b[32m   1325\u001b[39m             dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1326\u001b[39m             non_blocking,\n\u001b[32m   1327\u001b[39m             memory_format=convert_to_format,\n\u001b[32m   1328\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1329\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1333\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1334\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1335\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 42.06 MiB is free. Process 21724 has 352.70 MiB memory in use. Process 2999896 has 2.86 GiB memory in use. Including non-PyTorch memory, this process has 20.27 GiB memory in use. Of the allocated memory 19.90 GiB is allocated by PyTorch, and 30.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import FluxPriorReduxPipeline, FluxPipeline\n",
    "from diffusers.utils import load_image\n",
    "pipe_prior_redux = FluxPriorReduxPipeline.from_pretrained(rudex_id, torch_dtype=torch.bfloat16).to(\"cuda\")\n",
    "pipe = FluxPipeline.from_pretrained(\n",
    "    model_id , \n",
    "    text_encoder=None,\n",
    "    text_encoder_2=None,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/robot.png\")\n",
    "pipe_prior_output = pipe_prior_redux(image)\n",
    "images = pipe(\n",
    "    guidance_scale=2.5,\n",
    "    num_inference_steps=50,\n",
    "    generator=torch.Generator(\"cpu\").manual_seed(0),\n",
    "    **pipe_prior_output,\n",
    ").images\n",
    "images[0].save(\"flux-dev-redux.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtxt=\"**Character Status:** The girl stands confidently under a cherry blossom tree, her long auburn hair tousled by breeze, wearing a navy school uniform with a cream blouse. Observers (two classmates) linger nearby, one nodding quietly, the other adjusting their glasses thoughtfully.  **Action/Emotion:** She laughs softly at an unseen joke, cradling a manga; the admirers share a knowing glance, lips curved in quiet admiration, tension subtly charged.  **School Setting:** Overcast courtyard at dusk, surrounded by blooming sakura trees, stone benches, and scattered autumn leaves. The scene is secluded—near a quiet corner shaded by a lattice fence.  **Style:** Delicate linework with soft shading emphasizes textures (fabric, petals, skin). Dynamic angles: low camera tilt toward her, framing observers as diminished witnesses.  \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dictmanage/miniconda3/envs/talk2pic/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 11.90it/s]\n",
      "Loading pipeline components...:  71%|███████▏  | 5/7 [00:00<00:00,  7.84it/s]You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:00<00:00,  9.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model LoRA has been loaded: /home/dictmanage/.cache/modelscope/hub/models/lip421/ertongchahuaMAILANDFLUX/\n",
      "Model has been loaded: /home/dictmanage/.cache/modelscope/hub/models/black-forest-labs/FLUX.1-dev/\n",
      "Processing 1 dialogues...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (164 > 77). Running this sequence through the model will result in indexing errors\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['sun filtering through autumn maple trees ; cracked asphalt path, yellow school bus idling, \" riverbrook high \" sign, scattered backpacks nearby. ** style :** semi - realistic with muted texture, soft halftone sun rays, subtle motion blur on passing cars. ** colors :** soft ochre sunlight, navy uniforms, moss - green hedges, lily\\'s salmon scarf, pale gray concrete, lin ’ s coral shoe laces.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating image (Attempt 1)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:37<00:00,  3.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: park/image_000_attempt0_1742286164.png\n",
      "Image evaluation score: 0/10\n",
      "✅ Image is acceptable with score 0/10.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from llm.qwen import qwen,vqwen\n",
    "from llm.deepseek import deepseek,vdeepseek\n",
    "import os\n",
    "from PIL import Image\n",
    "import time\n",
    "from src.flux import flux\n",
    "\n",
    "class trans:\n",
    "    def __init__(self, api_key, model_id, lora_id=None, output_dir=\"output_images\"):\n",
    "        self.key = api_key\n",
    "        self.base_url = \"https://api.siliconflow.cn/v1\"\n",
    "        self.pipe = None\n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        try:\n",
    "            self.llm=qwen(api_key)\n",
    "        except:\n",
    "            self.llm=deepseek(api_key)\n",
    "        try:\n",
    "            self.vlm=vqwen(api_key)\n",
    "        except:\n",
    "            self.vlm=vdeepseek(api_key)\n",
    "        self.pipe=flux()\n",
    "\n",
    "    def generated_prompt(self, seed):\n",
    "        return self.llm.call_llm(seed)\n",
    "\n",
    "    def generate_image(self, prompt, index):\n",
    "        if self.pipe is None:\n",
    "            print(\"Error: FLUX Not Loaded!\")\n",
    "            return None\n",
    "\n",
    "        attempt = 0\n",
    "        while attempt < 5:\n",
    "            print(f\"Generating image (Attempt {attempt + 1})...\")\n",
    "            timestamp = int(time.time())\n",
    "            filename = os.path.join(self.output_dir, f\"image_{index:03d}_attempt{attempt}_{timestamp}.png\")\n",
    "            image=self.pipe.generate_image(prompt,index,attempt,filename)\n",
    "            image.save(filename)\n",
    "            print(f\"Saved: {filename}\")\n",
    "\n",
    "            score = self.vlm.evaluate_image(prompt, filename)\n",
    "            if score <= 6:\n",
    "                print(f\"✅ Image is acceptable with score {score}/10.\")\n",
    "                return filename\n",
    "            else:\n",
    "                print(f\"❌ Too many issues detected (score {score}/10). Refining prompt...\")\n",
    "                prompt = self.vlm.refine_prompt(prompt)\n",
    "\n",
    "            attempt += 1\n",
    "\n",
    "        print(\"⚠️ Maximum attempts reached. Using last generated image.\")\n",
    "        return filename\n",
    "\n",
    "    def batch_generate(self, text_file):\n",
    "        with open(text_file, \"r\", encoding=\"utf-8\") as file:\n",
    "            lines = [line.strip() for line in file.readlines() if line.strip()]\n",
    "\n",
    "        print(f\"Processing {len(lines)} dialogues...\")\n",
    "\n",
    "        for idx, line in enumerate(lines):\n",
    "            prompt = self.generated_prompt(line)\n",
    "            self.generate_image(prompt, idx)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    model_id=\"/home/dictmanage/.cache/modelscope/hub/models/black-forest-labs/FLUX___1-dev/\"\n",
    "    lora_id=\"/home/dictmanage/.cache/modelscope/hub/models/yiwanji/FLUX_xiao_hong_shu_ji_zhi_zhen_shi_V2/\"\n",
    "    lora_id1=\"/home/dictmanage/.cache/modelscope/hub/models/ChaosMY/MYkawaii4MJ/\"\n",
    "    lora_id2=\"/home/dictmanage/.cache/modelscope/hub/models/lip421/ertongchahuaMAILANDFLUX/\"\n",
    "    key=\"sk-qzvagazrsjcgrkqfmynotwtovonbqxcrfhmgpleutfovldgo\"\n",
    "    a=trans(key,model_id,lora_id2,\"park\")\n",
    "    a.batch_generate(\"/home/dictmanage/liuxh/talk2pic/tests/park/park.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1 dialogues...\n",
      "Generating image (Attempt 1)...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'timestamp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43ma\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/home/dictmanage/liuxh/talk2pic/tests/park/park.txt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 64\u001b[39m, in \u001b[36mtrans.batch_generate\u001b[39m\u001b[34m(self, text_file)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(lines):\n\u001b[32m     63\u001b[39m     prompt = \u001b[38;5;28mself\u001b[39m.generated_prompt(line)\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36mtrans.generate_image\u001b[39m\u001b[34m(self, prompt, index)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m attempt < \u001b[32m5\u001b[39m:\n\u001b[32m     37\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGenerating image (Attempt \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattempt\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     filename = os.path.join(\u001b[38;5;28mself\u001b[39m.output_dir, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mimage_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m03d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_attempt\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattempt\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mtimestamp\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.png\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     39\u001b[39m     image_file=\u001b[38;5;28mself\u001b[39m.pipe.generate_image(prompt,index,filename)\n\u001b[32m     40\u001b[39m     image.save(image_file)\n",
      "\u001b[31mNameError\u001b[39m: name 'timestamp' is not defined"
     ]
    }
   ],
   "source": [
    "a.batch_generate(\"/home/dictmanage/liuxh/talk2pic/tests/park/park.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "talk2pic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
